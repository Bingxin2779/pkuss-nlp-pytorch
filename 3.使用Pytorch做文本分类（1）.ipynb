{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Pytorch做文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类是自然语言处理中最基本的任务之一，我们可以根据已有的文本分类结果进行有监督训练后进行分类，也可以根据一定的要求将文本进行聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    def __init__(self, vocab_list):\n",
    "        self.vocab = self.load_vocab(vocab_list)\n",
    "\n",
    "    def load_vocab(self, vocab_list):\n",
    "        vocab = collections.OrderedDict()\n",
    "        vocab['UNK'] = 0\n",
    "        index = 1\n",
    "        for token in vocab_list:\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "        return vocab\n",
    "\n",
    "    def tokenize(self, str):\n",
    "        str = [s for s in str.strip()]\n",
    "        return str\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        if token not in self.vocab.keys():\n",
    "            return self.vocab['UNK']\n",
    "        else:\n",
    "            return self.vocab[token]\n",
    "\n",
    "    def tokens_to_ids(self, tokens):\n",
    "        ids_list = list(map(self.token_to_id, tokens))\n",
    "        return ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9110, 300)\n"
     ]
    }
   ],
   "source": [
    "char_list = []\n",
    "emb_list = []\n",
    "# noinspection JupyterKernel,JupyterKernel\n",
    "with open('素材\\sgns.wiki.char', 'r', encoding='utf-8') as emb_file:\n",
    "    dict_length, emb_size = emb_file.readline().rstrip().split()\n",
    "    dict_length, emb_size = int(dict_length), int(emb_size)\n",
    "    emb = collections.OrderedDict(get_coefs(*l.rstrip().split()) for l in emb_file.readlines())\n",
    "tokenizer = Tokenizer(emb.keys())\n",
    "emb_matrix = np.zeros((1 + dict_length, emb_size), dtype='float32')\n",
    "for word, id in tokenizer.vocab.items():\n",
    "    emb_vector = emb.get(word)\n",
    "    if emb_vector is not None:\n",
    "        emb_matrix[id] = emb_vector\n",
    "print(emb_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifierNet(\n",
      "  (emb): Embedding(9110, 300)\n",
      "  (relu): ReLU()\n",
      "  (linear1): Linear(in_features=1500, out_features=100, bias=True)\n",
      "  (linear2): Linear(in_features=100, out_features=20, bias=True)\n",
      "  (linear3): Linear(in_features=20, out_features=2, bias=True)\n",
      "  (softmax): Softmax(dim=-1)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# 初始化网络的实例\n",
    "seq_length = 5\n",
    "label_len = 2\n",
    "\n",
    "class LinearClassifierNet(nn.Module):\n",
    "    def __init__(self, seq_length, label_len):\n",
    "        super(LinearClassifierNet, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.label_len = label_len\n",
    "        self.emb = nn.Embedding.from_pretrained(torch.tensor(emb_matrix))\n",
    "        self.emb_size = self.emb.embedding_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear1 = nn.Linear(seq_length * emb_size, 100)\n",
    "        self.linear2 = nn.Linear(100, 20)\n",
    "        self.linear3 = nn.Linear(20, self.label_len)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1, self.seq_length * self.emb_size)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        if y is None:\n",
    "            return self.softmax(x)\n",
    "        loss = self.loss(x, y)\n",
    "        # y_hat\n",
    "        return loss\n",
    "\n",
    "\n",
    "net = LinearClassifierNet(seq_length, label_len)\n",
    "# 使用print可以打印出网络的结构\n",
    "print(net)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class sentiment_example():\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "class sentiment_feature():\n",
    "    def __init__(self, ids, label):\n",
    "        self.ids = ids\n",
    "        self.label = label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爱 1\n",
      "爱不忍释 1\n",
      "爱不释手 1\n",
      "[332, 0, 0, 0, 0] 1\n",
      "[332, 57, 2028, 2070, 0] 1\n",
      "[332, 57, 2070, 444, 0] 1\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "with open('素材/sentiment/正面情感词语（中文）.txt', 'r', encoding='gbk') as pos_file:\n",
    "    for line in pos_file:\n",
    "        line = line.strip()\n",
    "        examples.append(sentiment_example(line, 1))\n",
    "\n",
    "with open('素材/sentiment/负面情感词语（中文）.txt', 'r', encoding='gbk') as pos_file:\n",
    "    for line in pos_file:\n",
    "        line = line.strip()\n",
    "        examples.append(sentiment_example(line, 0))\n",
    "\n",
    "\n",
    "def convert_example_to_feature(examples):\n",
    "    features = []\n",
    "    for i in examples:\n",
    "        ids = tokenizer.tokens_to_ids(i.text)\n",
    "        if len(ids) > seq_length:\n",
    "            ids = ids[0: seq_length]\n",
    "        else:\n",
    "            ids = ids + [0] * (seq_length - len(ids))\n",
    "        if sum(ids) == 0:\n",
    "            continue\n",
    "        assert len(ids) == seq_length\n",
    "        features.append(sentiment_feature(ids, i.label))\n",
    "    return features\n",
    "\n",
    "for i in range(3):\n",
    "    print(examples[i].text, examples[i].label)\n",
    "\n",
    "features = convert_example_to_feature(examples)\n",
    "for i in range(3):\n",
    "    print(features[i].ids, features[i].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "ids = torch.tensor([f.ids for f in features], dtype=torch.long)\n",
    "label = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(ids, label)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.451051\n",
      "epoch: 2, loss: 0.276323\n",
      "epoch: 3, loss: 0.204856\n",
      "epoch: 4, loss: 0.147314\n",
      "epoch: 5, loss: 0.119233\n",
      "epoch: 6, loss: 0.083407\n",
      "epoch: 7, loss: 0.077122\n",
      "epoch: 8, loss: 0.059252\n",
      "epoch: 9, loss: 0.044757\n",
      "epoch: 10, loss: 0.036423\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "optimizer = Adam(net.parameters(), lr=0.002)\n",
    "\n",
    "epoch = 10\n",
    "for i in range(epoch):\n",
    "    total_loss = []\n",
    "    for ids, label in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            ids = ids.to(torch.device('cuda'))\n",
    "            label = label.to(torch.device('cuda'))\n",
    "        loss = net(ids, label)\n",
    "        total_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"epoch: %d, loss: %.6f\" % (i + 1, sum(total_loss) / len(total_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好\n",
      "positive\n",
      "再见\n",
      "positive\n",
      "凄凄惨惨戚戚\n",
      "negative\n",
      "杯具\n",
      "positive\n",
      "悲剧\n",
      "negative\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "def tensor_to_label(logits):\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    logits = np.argmax(logits, axis=-1)\n",
    "    if logits[0] == 1:\n",
    "        return 'positive'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "\n",
    "while True:\n",
    "    s = input()\n",
    "    if s == 'quit':\n",
    "        break\n",
    "    s = [sentiment_example(s, 0)]\n",
    "    s = convert_example_to_feature(s)\n",
    "    ids = torch.tensor([f.ids for f in s], dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        if torch.cuda.is_available():\n",
    "            ids = ids.to(torch.device('cuda'))\n",
    "        logits = tensor_to_label(net(ids))\n",
    "        print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
